{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP_Policy_Likelihood.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "XYTWctxrvpGi"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx2NJ1VN9DGG"
      },
      "source": [
        "# Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRt0pVU9H--",
        "outputId": "1eb01985-94d9-41e2-f992-d9c61a887fec",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "#Installs & Imports\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install torchvision\n",
        "import gym \n",
        "from gym import spaces\n",
        "import collections\n",
        "import pprint\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import operator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCSjGFN7MTfj"
      },
      "source": [
        "#Initialising MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps67o2Xl_GjO"
      },
      "source": [
        "#constants\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N = 1000\n",
        "\n",
        "class MDP:\n",
        "    def init(self):\n",
        "\n",
        "        self.S = [0, 1, 2]\n",
        "        self.endstate = self.S[-1]\n",
        "        self.gamma = 0.99 #for val_iteration_for_q\n",
        "        self.actions = [0, 1, 2] # 0 = BACK, 1 = FORWARD, 2 = STAY\n",
        "        self.currentstate = self.S[0]\n",
        "        self.rewards = {0: 0.0,\n",
        "                        1: 0.2,\n",
        "                        2: 1.0}\n",
        "        self.T = {\n",
        "            (0, 0): {0: 0.8, 1: 0.1, 2: 0.1},  \n",
        "            (0, 1): {0: 0.1, 1: 0.7, 2: 0.2}, \n",
        "            (0, 2): {0: 0.8, 1: 0.2}, \n",
        "            (1, 0): {0: 0.7, 1: 0.2, 2: 0.1}, \n",
        "            (1, 1): {0: 0.2, 1: 0.1, 2: 0.7}, \n",
        "            (1, 2): {0: 0.1, 1: 0.8, 2: 0.1},\n",
        "            (2, 0): {0: 0.2, 1: 0.7, 2: 0.1}, \n",
        "            (2, 1): {0: 0.7, 1: 0.2, 2: 0.1},\n",
        "            (2, 2): {0: 0.1, 1: 0.1, 2: 0.8}}\n",
        "\n",
        "        self.R = collections.defaultdict(float)\n",
        "        self.values = collections.defaultdict(float)\n",
        "        \n",
        "    #helper functions\n",
        "\n",
        "    def step(self, action):\n",
        "        isdone = False\n",
        "        #new state = state with highest probability from transition model given current state and action\n",
        "        new_state = max(self.T[(self.currentstate, action)].items(), key=operator.itemgetter(1))[0] \n",
        "        if new_state == self.endstate: \n",
        "            isdone = True \n",
        "        else:\n",
        "            isdone = False\n",
        "        return new_state, self.rewards[new_state], isdone\n",
        "       \n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None #initialise\n",
        "        for action in range(len(self.actions)): #for each action\n",
        "            action_value = self.values[(state, action)] #get utility\n",
        "            if best_value is None or best_value < action_value: #if action value > best value \n",
        "                best_value = action_value #set the value as best value\n",
        "                best_action = action #set the action as best action\n",
        "        return best_action #return action from state which yields highest utility \n",
        "\n",
        "    def get_state_utility(self, state):\n",
        "        utility = 0\n",
        "        for action in self.actions:\n",
        "            utility += self.values[state, action] #sum utilities of all possible actions from given state\n",
        "        return utility/len(agent.actions) #return average utility for that state \n",
        "\n",
        "    #functions\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = random.choice(self.actions) #select random action\n",
        "            new_state, reward, is_done = self.step(action) #take random action\n",
        "            self.R[(self.currentstate, action, new_state)] = reward #populate R with findings \n",
        "            self.currentstate = new_state #increment state\n",
        "        self.currentstate = self.S[0] #reset starting state\n",
        "\n",
        "    def value_iteration_for_Q(self):\n",
        "        for state in self.S: #for each state\n",
        "            for action in self.actions: #for each action\n",
        "                action_value = 0.0\n",
        "                target_probs = self.T[(state, action)] #get probabilites of target states\n",
        "                total = sum(target_probs.values()) #get total probability of all states\n",
        "                for tgt_state, probability in target_probs.items(): #for each tuple (S*, probability of landing in it given (S,A))\n",
        "                    key = (state, action, tgt_state) \n",
        "                    reward = self.R[key] #get reward for the transition\n",
        "                    best_action = self.select_action(tgt_state) #select action with highest utility\n",
        "                    val = reward + GAMMA * self.values[(tgt_state, best_action)] #generate Q value \n",
        "                    action_value += (probability / total) * val  #calculate updated action value\n",
        "                self.values[(state, action)] = action_value #update values with learned utility of (S,A)\n",
        "\n",
        "    def find_optimal_policy(self):\n",
        "        policy = [None] * len(self.S) #initialise empty policy\n",
        "        for state in self.S: #for each state\n",
        "            best_action = self.select_action(state) #select action with highest utility\n",
        "            policy[state] = best_action #set this as optimal action\n",
        "        return policy\n",
        "\n",
        "\n",
        "    def sample_paths(self, policy, no_paths):\n",
        "        paths = []\n",
        "        path = []\n",
        "        self.currentstate = self.S[0]\n",
        "        while (len(paths) != no_paths):\n",
        "            outcomes = np.array(list(self.T[self.currentstate, policy[self.currentstate]].values()))\n",
        "            newstatearray = np.random.multinomial(1, outcomes).tolist() #get new state array e.g [0,1,0]\n",
        "            new_state = newstatearray.index(1) #get actual state\n",
        "            path.append(policy[new_state]) #add action to path\n",
        "            if (self.rewards[new_state] == 1.0): \n",
        "                if (path not in paths and path != [1,1,2]):\n",
        "                    paths.append(path)\n",
        "                path = []\n",
        "                self.currentstate = self.S[0]\n",
        "            else:\n",
        "                self.currentstate = new_state\n",
        "\n",
        "        return paths"
      ],
      "execution_count": 107,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTDne9lbG0JJ"
      },
      "source": [
        "#Solving MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9kavHZ3cTaC",
        "outputId": "a0f9a6f4-64f0-431e-ed18-4dfdc55daacf",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 799
        }
      },
      "source": [
        "agent = MDP()\n",
        "agent.init()\n",
        "agent.play_n_random_steps(N)\n",
        "agent.value_iteration_for_Q()\n",
        "optimal_policy = agent.find_optimal_policy()\n",
        "paths = agent.sample_paths(optimal_policy, 5)\n",
        "paths.append(optimal_policy)\n",
        "print(\"Optimal Policy is {}\".format(optimal_policy))\n",
        "print(\"\\nLearned Reward Function:\\n\")\n",
        "pprint.pprint(agent.R)\n",
        "print(\"\\nLearned Values (Q):\\n\")\n",
        "pprint.pprint(agent.values)\n",
        "print(\"\\n Sampled Paths are: {}\".format(paths))"
      ],
      "execution_count": 108,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Optimal Policy is [1, 1, 2]\n",
            "\n",
            "Learned Reward Function:\n",
            "\n",
            "defaultdict(<class 'float'>,\n",
            "            {(0, 0, 0): 0.0,\n",
            "             (0, 0, 1): 0.0,\n",
            "             (0, 0, 2): 0.0,\n",
            "             (0, 1, 0): 0.0,\n",
            "             (0, 1, 1): 0.2,\n",
            "             (0, 1, 2): 0.0,\n",
            "             (0, 2, 0): 0.0,\n",
            "             (0, 2, 1): 0.0,\n",
            "             (1, 0, 0): 0.0,\n",
            "             (1, 0, 1): 0.0,\n",
            "             (1, 0, 2): 0.0,\n",
            "             (1, 1, 0): 0.0,\n",
            "             (1, 1, 1): 0.0,\n",
            "             (1, 1, 2): 1.0,\n",
            "             (1, 2, 0): 0.0,\n",
            "             (1, 2, 1): 0.2,\n",
            "             (1, 2, 2): 0.0,\n",
            "             (2, 0, 0): 0.0,\n",
            "             (2, 0, 1): 0.2,\n",
            "             (2, 0, 2): 0.0,\n",
            "             (2, 1, 0): 0.0,\n",
            "             (2, 1, 1): 0.0,\n",
            "             (2, 1, 2): 0.0,\n",
            "             (2, 2, 0): 0.0,\n",
            "             (2, 2, 1): 0.0,\n",
            "             (2, 2, 2): 1.0})\n",
            "\n",
            "Learned Values (Q):\n",
            "\n",
            "defaultdict(<class 'float'>,\n",
            "            {(0, 0): 0.0,\n",
            "             (0, 1): 0.13999999999999999,\n",
            "             (0, 2): 0.1008,\n",
            "             (1, 0): 0.08820000000000001,\n",
            "             (1, 1): 0.733138,\n",
            "             (1, 2): 0.70045936,\n",
            "             (2, 0): 0.6270769400000001,\n",
            "             (2, 1): 0.2766017646000001,\n",
            "             (2, 2): 1.3300778168000003})\n",
            "\n",
            " Sampled Paths are: [[2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twhX9COB8O6"
      },
      "source": [
        "#Likelihood Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm8KOzjPYS0K",
        "outputId": "a70dabe5-bec3-4809-b201-39828c7549da",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 272
        }
      },
      "source": [
        "def likelihood(paths, agent):\n",
        "    likelihoodList = []\n",
        "    for path in paths:\n",
        "        actionLikelihoodList = [None] * len(path)\n",
        "        for i in range(len(path)):\n",
        "            state = int(i)\n",
        "            action = int(path[i])    \n",
        "            actionLikelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "        likelihoodList.append(sum(actionLikelihoodList) / len(actionLikelihoodList))\n",
        "    return likelihoodList\n",
        "\n",
        "likelihoods = likelihood(paths, agent) #function call\n",
        "\n",
        "#formatting output\n",
        "print(\"The paths are: {}\".format(paths))\n",
        "for l in likelihoods:\n",
        "    index = likelihoods.index(l)\n",
        "    value = likelihoods[index]\n",
        "    formatlist = [index+1, value]\n",
        "    print(\"\\nLikelihood for path {} is {}\".format(*formatlist))\n",
        "\n",
        "print(\"\\nThe optimal policy is path {}\".format(likelihoods.index(max(likelihoods))+1))\n",
        "\n",
        "#take R as input and compute values in function\n"
      ],
      "execution_count": 114,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The paths are: [[2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2]]\n",
            "\n",
            "Likelihood for path 1 is 0.020533333333333334\n",
            "\n",
            "Likelihood for path 2 is 0.1264634533333333\n",
            "\n",
            "Likelihood for path 3 is -0.036475639173333366\n",
            "\n",
            "Likelihood for path 4 is -0.045594548966666704\n",
            "\n",
            "Likelihood for path 5 is -0.0303963659777778\n",
            "\n",
            "Likelihood for path 6 is 0.2903659521111111\n",
            "\n",
            "The optimal policy is path 6\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYTWctxrvpGi"
      },
      "source": [
        "# **ignore** *Copy Paste Dump* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzvrDalNvrfq"
      },
      "source": [
        "\"\"\"\n",
        "threshold = 0.0001\n",
        "action = 0\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =100\n",
        "\n",
        "        #hard coded R\n",
        "        self.R_hardcoded = {(0, 0, 0): 0.0, \n",
        "                  (0, 0, 1): 0.0, \n",
        "                  (0, 0, 2): 1.0,\n",
        "                  (0, 1, 0): 0.0,\n",
        "                  (0, 1, 1): 0.0, \n",
        "                  (0, 1, 2): 1.0,\n",
        "                  (0, 2, 0): 0.0,\n",
        "                  (0, 2, 1): 0.0,\n",
        "                  (1, 0, 0): 0.0, \n",
        "                  (1, 0, 1): 0.0, \n",
        "                  (1, 0, 2): 1.0,\n",
        "                  (1, 1, 0): 0.0,\n",
        "                  (1, 1, 1): 0.0, \n",
        "                  (1, 1, 2): 1.0,\n",
        "                  (1, 2, 0): 0.0,\n",
        "                  (1, 2, 1): 0.0,\n",
        "                  (1, 2, 2): 1.0,\n",
        "                  (2, 0, 0): 0.0, \n",
        "                  (2, 0, 1): 0.0, \n",
        "                  (2, 0, 2): 1.0,\n",
        "                  (2, 1, 0): 0.0,\n",
        "                  (2, 1, 1): 0.0, \n",
        "                  (2, 1, 2): 1.0,\n",
        "                  (2, 2, 0): 0.0,\n",
        "                  (2, 2, 1): 0.0,\n",
        "                  (2, 2, 2): 1.0}\n",
        "\n",
        "        #hard coded T\n",
        "\n",
        "def step(self, action):\n",
        "        isdone = False\n",
        "        if action == 2:\n",
        "            currentstate = self.currentstate #remain at current state\n",
        "        elif action == 1:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)+1] #take step to right\n",
        "            except IndexError:\n",
        "                currentstate = self.actions[0]\n",
        "        else:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)-1] #take step to left\n",
        "            except IndexError:\n",
        "                currentstate = self.currentstate\n",
        "        if currentstate == self.endstate:\n",
        "            isdone = True\n",
        "        else:\n",
        "            isdone = False\n",
        "\n",
        "        return currentstate, self.rewards[currentstate], isdone\n",
        "        \n",
        "        \n",
        "        #single path function\n",
        "        def likelihood1(policy, agent):\n",
        "            likelihoodList = [None] * len(policy)\n",
        "            for i in range(len(policy)):\n",
        "                state = int(i)\n",
        "                action = int(policy[i])    \n",
        "                likelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "            return sum(likelihoodList) / len(likelihoodList)\n",
        "\n",
        "            #create random policies for testing\n",
        "actions = [0, 1, 2, 3]\n",
        "nonoptimal1 = [None] * 16\n",
        "nonoptimal2 = [None] * 16\n",
        "nonoptimal3 = [None] * 16\n",
        "policies = [nonoptimal1, nonoptimal2, nonoptimal3]\n",
        "for policy in policies:\n",
        "    for i in range(len(policy)):\n",
        "        policy[i] = random.choice(actions)\n",
        "\n",
        "#create random reward functions for testing\n",
        "nonoptimal_rewardfunc1 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc2 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc3 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "\n",
        "\n",
        "#get optimal reward function\n",
        "q_table = create_q_table(env) \n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal policy with random reward functions ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal reward function is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal reward function 1 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc1)))\n",
        "print(\"Likelihood for non optimal reward function 2 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc2)))\n",
        "print(\"Likelihood for non optimal reward function 3 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc3)))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal reward function with random policies ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal policy  is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal policy 1 is {}\".format(likelihood(nonoptimal1, q_table)))\n",
        "print(\"Likelihood for non optimal policy 2 is {}\".format(likelihood(nonoptimal2, q_table)))\n",
        "print(\"Likelihood for non optimal policy 3 is {}\".format(likelihood(nonoptimal3, q_table)))\n",
        "print(\"\\n\")\n",
        "                \n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Plr6WNpv6WZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}