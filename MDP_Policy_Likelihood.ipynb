{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP_Policy_Likelihood.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Sx2NJ1VN9DGG",
        "CCSjGFN7MTfj",
        "VZfaPY8sQ4Cd",
        "XYTWctxrvpGi"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx2NJ1VN9DGG"
      },
      "source": [
        "# Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRt0pVU9H--",
        "outputId": "33eed165-afcf-4fb6-f75c-dba8bd26bc7b",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        }
      },
      "source": [
        "#Installs & Imports\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install torchvision\n",
        "import gym \n",
        "from gym import spaces\n",
        "import collections\n",
        "import pprint\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import operator"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCSjGFN7MTfj"
      },
      "source": [
        "#MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps67o2Xl_GjO"
      },
      "source": [
        "#constants\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N = 1000\n",
        "\n",
        "class MDP:\n",
        "    def init(self):\n",
        "\n",
        "        self.S = [0, 1, 2]\n",
        "        self.endstate = self.S[-1]\n",
        "        self.gamma = 0.99 #for val_iteration_for_q\n",
        "        self.actions = [0, 1, 2] # 0 = BACK, 1 = FORWARD, 2 = STAY\n",
        "        self.currentstate = self.S[0]\n",
        "        self.rewards = {0: -0.9641254941220043, 1: -0.9971473606992696, 2: 1.0}#true optimal reward function\n",
        "        self.T = {\n",
        "            (0, 0): {0: 0.7, 1: 0.2, 2: 0.1},  \n",
        "            (0, 1): {0: 0.1, 1: 0.7, 2: 0.2}, \n",
        "            (0, 2): {0: 0.7, 1: 0.2, 2: 0.1}, \n",
        "            (1, 0): {0: 0.7, 1: 0.2, 2: 0.1}, \n",
        "            (1, 1): {0: 0.1, 1: 0.2, 2: 0.7}, \n",
        "            (1, 2): {0: 0.1, 1: 0.7, 2: 0.2},\n",
        "            (2, 0): {0: 0.2, 1: 0.7, 2: 0.1}, \n",
        "            (2, 1): {0: 0.7, 1: 0.2, 2: 0.1},\n",
        "            (2, 2): {0: 0.1, 1: 0.1, 2: 0.8}}\n",
        "\n",
        "        self.R = collections.defaultdict(float)\n",
        "        self.values = collections.defaultdict(float)\n",
        "        \n",
        "    #helper functions\n",
        "\n",
        "    def step(self, action):\n",
        "        isdone = False\n",
        "        #new state = state with highest probability from transition model given current state and action\n",
        "        new_state = max(self.T[(self.currentstate, action)].items(), key=operator.itemgetter(1))[0] \n",
        "        if new_state == self.endstate: \n",
        "            isdone = True \n",
        "        else:\n",
        "            isdone = False\n",
        "        return new_state, self.rewards[new_state], isdone\n",
        "       \n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None #initialise\n",
        "        for action in range(len(self.actions)): #for each action\n",
        "            action_value = self.values[(state, action)] #get utility\n",
        "            if best_value is None or best_value < action_value: #if action value > best value \n",
        "                best_value = action_value #set the value as best value\n",
        "                best_action = action #set the action as best action\n",
        "        return best_action #return action from state which yields highest utility \n",
        "\n",
        "    def get_state_utility(self, state):\n",
        "        utility = 0\n",
        "        for action in self.actions:\n",
        "            utility += self.values[state, action] #sum utilities of all possible actions from given state\n",
        "        return utility/len(agent.actions) #return average utility for that state \n",
        "\n",
        "    #functions\n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = random.choice(self.actions) #select random action\n",
        "            new_state, reward, is_done = self.step(action) #take random action\n",
        "            self.R[(self.currentstate, action, new_state)] = reward #populate R with findings \n",
        "            self.currentstate = new_state #increment state\n",
        "        self.currentstate = self.S[0] #reset starting state\n",
        "\n",
        "    def value_iteration_for_Q(self):\n",
        "        for state in self.S: #for each state\n",
        "            for action in self.actions: #for each action\n",
        "                action_value = 0.0\n",
        "                target_probs = self.T[(state, action)] #get probabilites of target states\n",
        "                total = sum(target_probs.values()) #get total probability of all states\n",
        "                for tgt_state, probability in target_probs.items(): #for each tuple (S*, probability of landing in it given (S,A))\n",
        "                    key = (state, action, tgt_state) \n",
        "                    reward = self.R[key] #get reward for the transition\n",
        "                    best_action = self.select_action(tgt_state) #select action with highest utility\n",
        "                    val = reward + GAMMA * self.values[(tgt_state, best_action)] #generate Q value \n",
        "                    action_value += (probability / total) * val  #calculate updated action value\n",
        "                self.values[(state, action)] = action_value #update values with learned utility of (S,A)\n",
        "\n",
        "    def find_optimal_policy(self):\n",
        "        policy = [None] * len(self.S) #initialise empty policy\n",
        "        for state in self.S: #for each state\n",
        "            best_action = self.select_action(state) #select action with highest utility\n",
        "            policy[state] = best_action #set this as optimal action\n",
        "        return policy\n",
        "\n",
        "\n",
        "    def sample_paths(self, policy, no_paths):\n",
        "        paths = []\n",
        "        path = []\n",
        "        self.currentstate = self.S[0]\n",
        "        while (len(paths) != no_paths):\n",
        "            outcomes = self.T[self.currentstate, policy[self.currentstate]].values() #get probability of target states given action from policy as dict value obj\n",
        "            probs = [] #empty list\n",
        "            for item in outcomes:\n",
        "                probs.append(item) #populate probs list with outcomes \n",
        "            probs = np.array(probs) #cast probs list to np.array\n",
        "            newstatearray = np.random.multinomial(1, probs).tolist() #get new state array e.g [0,1,0]\n",
        "            new_state = newstatearray.index(1) #get actual state\n",
        "            path.append(policy[new_state]) #add action to path\n",
        "            endstate = self.endstate\n",
        "            if (new_state == endstate): #if at terminal state\n",
        "                #if (path not in paths and path != [1,1,2]): \n",
        "                paths.append(path) #add path to paths\n",
        "                path = [] #reset path\n",
        "                self.currentstate = self.S[0] #reset to start state\n",
        "            else:\n",
        "                self.currentstate = new_state \n",
        "\n",
        "        return paths"
      ],
      "execution_count": 99,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twhX9COB8O6"
      },
      "source": [
        "#Likelihood & Hillclimb Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P69SYhk5WPNC"
      },
      "source": [
        "def likelihood(paths, rewards):\n",
        "    #initialise MDP with given reward function\n",
        "    agent = MDP()\n",
        "    agent.init()\n",
        "    agent.rewards = rewards\n",
        "    agent.play_n_random_steps(N)\n",
        "    agent.value_iteration_for_Q()\n",
        "    agent.find_optimal_policy()\n",
        "    likelihoodList = [] \n",
        "    for path in paths:\n",
        "        actionLikelihoodList = [None] * len(path) #empty list length of path\n",
        "        state = agent.S[0]\n",
        "        i = 0\n",
        "        for step in path:\n",
        "            action = step \n",
        "            state = max(agent.T[(state, action)].items(), key=operator.itemgetter(1))[0]  #update state given action from path\n",
        "            actionLikelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) #Q^Rstate,action −V^Rstate\n",
        "            i += 1\n",
        "        likelihoodList.append(sum(actionLikelihoodList) / len(actionLikelihoodList))\n",
        "    return sum(likelihoodList)"
      ],
      "execution_count": 103,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlPDX_JYgNCX"
      },
      "source": [
        "def stochastic_hillclimb(paths, threshold):\n",
        "    Start = -1.0 #lower reward limit\n",
        "    Stop = 1.0 #upper reward limit\n",
        "    limit = 2 #number of states we are estimating likelihoods for (0 & 1), terminal state is set at 1.\n",
        "    likelihood_threshold = threshold\n",
        "    threshold_reached = False \n",
        "    final_rewards = collections.defaultdict(float) #reward dict to be returned\n",
        "\n",
        "    while not threshold_reached:\n",
        "        RandomListOfIntegers = [random.uniform(Start, Stop) for iter in range(limit)] #random list of reward ints \n",
        "        rewards = {0: RandomListOfIntegers[0], 1: RandomListOfIntegers[1], 2: 1.0} #map to dictionary {state:rewards}\n",
        "\n",
        "        HC_likelihood_list = []\n",
        "        for i in range(100):\n",
        "            HC_likelihood_list.append(likelihood(paths, rewards)) \n",
        "\n",
        "        rewards_likelihood = sum(HC_likelihood_list)/len(HC_likelihood_list) #average likelihood in list of 100\n",
        "        \n",
        "        \"\"\"\n",
        "        #un-comment to print reward func and corresponding likelihood func on each iteration\n",
        "        formatlist = [rewards, rewards_likelihood]\n",
        "        print(\"Rewards: {},   likeihood: {}\".format(*formatlist))\n",
        "        \"\"\"\n",
        "\n",
        "        if(rewards_likelihood > likelihood_threshold): #once threshold exceeded\n",
        "            #terminate and return\n",
        "            threshold_reached = True\n",
        "            final_rewards = rewards\n",
        "\n",
        "    return final_rewards"
      ],
      "execution_count": 106,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mMJnw-bYA0Q"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAc40mV0YAg_",
        "outputId": "10d86f6a-64c3-4b2a-8b28-5449eb536088",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 411
        }
      },
      "source": [
        "agent = MDP()\n",
        "agent.init()\n",
        "agent.play_n_random_steps(N)\n",
        "agent.value_iteration_for_Q()\n",
        "optimal_policy = agent.find_optimal_policy()\n",
        "no_of_paths = 1000\n",
        "paths = agent.sample_paths(optimal_policy, no_of_paths)\n",
        "main_likelihoods_list = []\n",
        "for i in range(100):\n",
        "    main_likelihoods_list.append(likelihood(paths, agent.rewards)) \n",
        "printlist = [agent.rewards, sum(main_likelihoods_list)/len(main_likelihoods_list)]\n",
        "print(\"\\nThe optimal reward function {} has likelihood of {} (avg of 100 samples)\".format(*printlist))\n",
        "print(\"\\nTrue Optimal Policy is {}\".format(optimal_policy))\n",
        "formatlist = [no_of_paths, paths]\n",
        "print(\"\\nThe {} sampled paths are: {}\".format(*formatlist))\n",
        "optimal_rewards = stochastic_hillclimb(paths, 270)\n",
        "formatlist = [optimal_rewards, likelihood(paths, optimal_rewards)]\n",
        "print(\"\\nHill climb converges at reward function {} with a likelihood of {} (avg of 100 samples)\".format(*formatlist))\n",
        "agent.rewards = optimal_rewards\n",
        "agent.play_n_random_steps(N)\n",
        "agent.value_iteration_for_Q()\n",
        "print(\"\\nThe optimal policy under the optimal reward func is: {}\".format(agent.find_optimal_policy()))\n",
        "agent.value_iteration_for_Q()\n",
        "print(\"\\nThe Q under the optimal reward func is\")\n",
        "pprint.pprint(agent.values)"
      ],
      "execution_count": 110,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The optimal reward function {0: -0.999753592635372, 1: -0.9374563633938111, 2: 1.0} has likelihood of 279.42164671115745 (avg of 100 samples)\n",
            "\n",
            "True Optimal Policy is [1, 1, 2]\n",
            "\n",
            "The 1000 sampled paths are: [[1, 1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 1, 2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 1, 2], [2], [1, 2], [2], [1, 2], [2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 2], [2], [1, 2], [1, 1, 2], [2], [2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [2], [2], [2], [1, 2], [2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [2], [2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [2], [2], [2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 1, 1, 2], [2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [2], [2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [2], [2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 2]]\n",
            "\n",
            "Hill climb converges at reward function {0: -0.9861644761502912, 1: -0.9260095198987501, 2: 1.0} with a likelihood of 277.33144009854965\n",
            "\n",
            "The optimal policy under the optimal reward func is: [1, 1, 2]\n",
            "\n",
            "The Q under the optimal reward func is\n",
            "defaultdict(<class 'float'>,\n",
            "            {(0, 0): -0.42317611101399943,\n",
            "             (0, 1): 0.4250601827711362,\n",
            "             (0, 2): -0.05476359983100093,\n",
            "             (1, 0): -0.05476359983100093,\n",
            "             (1, 1): 1.9041425354100583,\n",
            "             (1, 2): 0.8556997167060367,\n",
            "             (2, 0): 0.7609345497167277,\n",
            "             (2, 1): 0.05323902165313402,\n",
            "             (2, 2): 2.0737929121459993})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZfaPY8sQ4Cd"
      },
      "source": [
        "#Testing Reward Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "r0GB6ssOQ936",
        "outputId": "5aa713d5-6388-4559-c674-08bf25eabcfd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#edit to test diff reward functions    \n",
        "agent = MDP()\n",
        "agent.init()\n",
        "agent.play_n_random_steps(N)\n",
        "agent.value_iteration_for_Q()\n",
        "test_likelihoods_list =[]\n",
        "rewardfunc = {0: -0.999753592635372, 1: -0.9374563633938111, 2: 1.0}\n",
        "for i in range(100):\n",
        "    test_likelihoods_list.append(likelihood(agent.sample_paths(agent.find_optimal_policy(), 1000), rewardfunc)) \n",
        "agent.rewards = rewardfunc\n",
        "print(agent.find_optimal_policy())\n",
        "print(sum(test_likelihoods_list)/len(test_likelihoods_list))"
      ],
      "execution_count": 102,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 2]\n",
            "284.58953901567367\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYTWctxrvpGi"
      },
      "source": [
        "# **ignore** *Copy Paste Dump* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzvrDalNvrfq"
      },
      "source": [
        "\"\"\"\n",
        "threshold = 0.0001\n",
        "action = 0\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =100\n",
        "\n",
        "        #hard coded R\n",
        "        self.R_hardcoded = {(0, 0, 0): 0.0, \n",
        "                  (0, 0, 1): 0.0, \n",
        "                  (0, 0, 2): 1.0,\n",
        "                  (0, 1, 0): 0.0,\n",
        "                  (0, 1, 1): 0.0, \n",
        "                  (0, 1, 2): 1.0,\n",
        "                  (0, 2, 0): 0.0,\n",
        "                  (0, 2, 1): 0.0,\n",
        "                  (1, 0, 0): 0.0, \n",
        "                  (1, 0, 1): 0.0, \n",
        "                  (1, 0, 2): 1.0,\n",
        "                  (1, 1, 0): 0.0,\n",
        "                  (1, 1, 1): 0.0, \n",
        "                  (1, 1, 2): 1.0,\n",
        "                  (1, 2, 0): 0.0,\n",
        "                  (1, 2, 1): 0.0,\n",
        "                  (1, 2, 2): 1.0,\n",
        "                  (2, 0, 0): 0.0, \n",
        "                  (2, 0, 1): 0.0, \n",
        "                  (2, 0, 2): 1.0,\n",
        "                  (2, 1, 0): 0.0,\n",
        "                  (2, 1, 1): 0.0, \n",
        "                  (2, 1, 2): 1.0,\n",
        "                  (2, 2, 0): 0.0,\n",
        "                  (2, 2, 1): 0.0,\n",
        "                  (2, 2, 2): 1.0}\n",
        "\n",
        "        #hard coded T\n",
        "\n",
        "def step(self, action):\n",
        "        isdone = False\n",
        "        if action == 2:\n",
        "            currentstate = self.currentstate #remain at current state\n",
        "        elif action == 1:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)+1] #take step to right\n",
        "            except IndexError:\n",
        "                currentstate = self.actions[0]\n",
        "        else:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)-1] #take step to left\n",
        "            except IndexError:\n",
        "                currentstate = self.currentstate\n",
        "        if currentstate == self.endstate:\n",
        "            isdone = True\n",
        "        else:\n",
        "            isdone = False\n",
        "\n",
        "        return currentstate, self.rewards[currentstate], isdone\n",
        "        \n",
        "        \n",
        "        #single path function\n",
        "        def likelihood1(policy, agent):\n",
        "            likelihoodList = [None] * len(policy)\n",
        "            for i in range(len(policy)):\n",
        "                state = int(i)\n",
        "                action = int(policy[i])    \n",
        "                likelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "            return sum(likelihoodList) / len(likelihoodList)\n",
        "\n",
        "            #create random policies for testing\n",
        "actions = [0, 1, 2, 3]\n",
        "nonoptimal1 = [None] * 16\n",
        "nonoptimal2 = [None] * 16\n",
        "nonoptimal3 = [None] * 16\n",
        "policies = [nonoptimal1, nonoptimal2, nonoptimal3]\n",
        "for policy in policies:\n",
        "    for i in range(len(policy)):\n",
        "        policy[i] = random.choice(actions)\n",
        "\n",
        "#create random reward functions for testing\n",
        "nonoptimal_rewardfunc1 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc2 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc3 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "\n",
        "\n",
        "#get optimal reward function\n",
        "q_table = create_q_table(env) \n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal policy with random reward functions ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal reward function is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal reward function 1 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc1)))\n",
        "print(\"Likelihood for non optimal reward function 2 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc2)))\n",
        "print(\"Likelihood for non optimal reward function 3 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc3)))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal reward function with random policies ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal policy  is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal policy 1 is {}\".format(likelihood(nonoptimal1, q_table)))\n",
        "print(\"Likelihood for non optimal policy 2 is {}\".format(likelihood(nonoptimal2, q_table)))\n",
        "print(\"Likelihood for non optimal policy 3 is {}\".format(likelihood(nonoptimal3, q_table)))\n",
        "print(\"\\n\")\n",
        "                \n",
        "\"\"\"\n",
        "\n",
        "def likelihood2(paths, agent):\n",
        "    likelihoodList = []\n",
        "    for path in paths:\n",
        "        actionLikelihoodList = [None] * len(path)\n",
        "        for i in range(len(path)):\n",
        "            state = int(i)\n",
        "            action = int(path[i])    \n",
        "            actionLikelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "        likelihoodList.append(sum(actionLikelihoodList) / len(actionLikelihoodList))\n",
        "    return likelihoodList\n",
        "\n",
        "\"\"\"\n",
        "likelihoods = likelihood(paths, agent) #function call\n",
        "\n",
        "#formatting output\n",
        "print(\"The paths are: {}\".format(paths))\n",
        "for l in likelihoods:\n",
        "    index = likelihoods.index(l)\n",
        "    value = likelihoods[index]\n",
        "    formatlist = [index+1, value]\n",
        "    print(\"\\nLikelihood for path {} is {}\".format(*formatlist))\n",
        "\n",
        "print(\"\\nThe optimal policy is path {}\".format(likelihoods.index(max(likelihoods))+1))\n",
        "\n",
        "#take R as input and compute values in function\n",
        "\"\"\"\n",
        "\n",
        "def likelihood2(paths, rewards):\n",
        "    listoflikelihoodlists = []\n",
        "    for reward in rewards:\n",
        "        agent = MDP()\n",
        "        agent.init()\n",
        "        agent.rewards = reward\n",
        "        agent.play_n_random_steps(N)\n",
        "        agent.value_iteration_for_Q()\n",
        "        agent.find_optimal_policy()\n",
        "        likelihoodList = []\n",
        "        for path in paths:\n",
        "            actionLikelihoodList = [None] * len(path)\n",
        "            for i in range(len(path)):\n",
        "                state = int(i)\n",
        "                action = int(path[i])    \n",
        "                actionLikelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "            likelihoodList.append(sum(actionLikelihoodList) / len(actionLikelihoodList))\n",
        "        listoflikelihoodlists.append(likelihoodList)\n",
        "    \n",
        "    \n",
        "    return listoflikelihoodlists\n",
        "\"\"\"\n",
        "#0: 0.0, 1: 0.2, 2: 1.0\n",
        "likelihoods = likelihood(paths, {0: 0.0, 1: 0.2, 2: 1.0}) #function call\n",
        "formatlist = [paths, {0: 0.0, 1: 0.2, 2: 1.0}, likelihoods]\n",
        "print(\"The likelihoods of paths: \\n{}\\nunder the rewards: \\n{}\\nare: \\n\\n{}\\n\\n\".format(*formatlist))\n",
        "\n",
        "#0: 0.0, 1: 0.0, 2: 1.0\n",
        "likelihoods = likelihood(paths, {0: 0.0, 1: 0.0, 2: 1.0}) #function call\n",
        "formatlist = [paths, {0: 0.0, 1: 0.0, 2: 1.0}, likelihoods]\n",
        "print(\"The likelihoods of paths: \\n{}\\nunder the rewards: \\n{}\\nare: \\n\\n{}\\n\\n\".format(*formatlist))\n",
        "\n",
        "#0: 0.0, 1: 0.8, 2: 1.0\n",
        "likelihoods = likelihood(paths, {0: 0.0, 1: 0.8, 2: 1.0}) #function call\n",
        "formatlist = [paths, {0: 0.0, 1: 0.8, 2: 1.0}, likelihoods]\n",
        "print(\"The likelihoods of paths: \\n{}\\nunder the rewards: \\n{}\\nare: \\n\\n{}\\n\\n\".format(*formatlist))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rewards = [{0: 0.0, 1: 0.2, 2: 1.0},\n",
        "{0: 0.0, 1: 0.0, 2: 1.0},\n",
        "{0: 0.0, 1: 0.8, 2: 1.0},\n",
        "{0: 0.0, 1: 1.0, 2: 2.0}]\n",
        "\n",
        "\n",
        "listoflikelihoodlists = likelihood1(paths, rewards)\n",
        "\n",
        "print(\"\\n\\n\\nSum of likelihoods for each reward func are:\")\n",
        "\n",
        "for list in listoflikelihoodlists:\n",
        "    print(sum(list))\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Plr6WNpv6WZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}