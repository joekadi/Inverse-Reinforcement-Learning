{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP_Policy_Likelihood.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Sx2NJ1VN9DGG",
        "-twhX9COB8O6",
        "VZfaPY8sQ4Cd",
        "XYTWctxrvpGi"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx2NJ1VN9DGG"
      },
      "source": [
        "# Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRt0pVU9H--",
        "outputId": "2a80524c-837e-4ca9-980c-d49eaef1bd63",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 297
        }
      },
      "source": [
        "#Installs & Imports\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install torchvision\n",
        "import gym \n",
        "import math\n",
        "from gym import spaces\n",
        "import collections\n",
        "import pprint\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import operator\n",
        "import matplotlib.pyplot as plt\n",
        "import copy"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from cycler>=0.10->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCSjGFN7MTfj"
      },
      "source": [
        "#MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps67o2Xl_GjO"
      },
      "source": [
        "#constants\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N = 1000\n",
        "\n",
        "class MDP:\n",
        "    def init(self):\n",
        "        self.S = [0, 1, 2]\n",
        "        self.endstate = self.S[-1]\n",
        "        self.gamma = 0.99 #for val_iteration_for_q\n",
        "        self.actions = [0, 1, 2] # 0 = BACK, 1 = FORWARD, 2 = STAY\n",
        "        self.currentstate = self.S[0]\n",
        "        self.rewards = {0: -0.5, 1: 0, 2: 1.5} #true optimal reward function\n",
        "        self.T = {\n",
        "            (0, 0): {0: 0.7, 1: 0.2, 2: 0.1},  \n",
        "            (0, 1): {0: 0.1, 1: 0.7, 2: 0.2}, \n",
        "            (0, 2): {0: 0.7, 1: 0.2, 2: 0.1}, \n",
        "            (1, 0): {0: 0.7, 1: 0.2, 2: 0.1}, \n",
        "            (1, 1): {0: 0.1, 1: 0.2, 2: 0.7}, \n",
        "            (1, 2): {0: 0.1, 1: 0.7, 2: 0.2},\n",
        "            (2, 0): {0: 0.2, 1: 0.7, 2: 0.1}, \n",
        "            (2, 1): {0: 0.7, 1: 0.2, 2: 0.1},\n",
        "            (2, 2): {0: 0.1, 1: 0.2, 2: 0.7}}\n",
        "        self.Q = collections.defaultdict(float)\n",
        "        self.V = collections.defaultdict(float)\n",
        "      \n",
        "        \n",
        "    #helper functions\n",
        "\n",
        "    \n",
        "    def normalise_Q(self):\n",
        "        #normalises self.Q\n",
        "        current_values = list(self.Q.values())\n",
        "        normalized_values = NormalizeData(current_values)\n",
        "        i = 0\n",
        "        for state in self.S: #for each state\n",
        "            for action in self.actions: #for each action\n",
        "                self.Q[(state, action)] = normalized_values[i]\n",
        "                i += 1\n",
        "\n",
        "    def step(self, action):\n",
        "        isdone = False\n",
        "        #new state = state with highest probability from transition model given current state and action\n",
        "        new_state = max(self.T[(self.currentstate, action)].items(), key=operator.itemgetter(1))[0] \n",
        "        if new_state == self.endstate: \n",
        "            isdone = True \n",
        "        else:\n",
        "            isdone = False\n",
        "        return new_state, self.rewards[new_state], isdone\n",
        "       \n",
        "    def select_action(self, state):\n",
        "        best_action, best_value = None, None #initialise\n",
        "        for action in range(len(self.actions)): #for each action\n",
        "            action_value = self.Q[(state, action)] #get utility\n",
        "            if best_value is None or best_value < action_value: #if action value > best value \n",
        "                best_value = action_value #set the value as best value\n",
        "                best_action = action #set the action as best action\n",
        "        return best_action #return action from state which yields highest utility \n",
        "\n",
        "    def get_state_utility(self, state):\n",
        "        utility = 0\n",
        "        for action in self.actions:\n",
        "            utility += math.exp(self.Q[state, action])#sum utilities of all possible actions from given state\n",
        "\n",
        "        return math.log(utility) #return log of Q of all action in  state \n",
        "\n",
        "    #functions\n",
        "    def value_iteration_for_Q(self):\n",
        "        for state in self.S:\n",
        "            self.V[state] = 0\n",
        "        i = 1\n",
        "        converged = False\n",
        "        while not converged:\n",
        "            Vp = copy.copy(self.V)\n",
        "            for state in self.S: \n",
        "                for action in self.actions: \n",
        "                    #Q(s,a) = r(s) + GAMMA * sum(T(s' a | s) * Vp(s'))\n",
        "                    q = self.rewards[state]\n",
        "                    target_probs = self.T[state,action]\n",
        "                    sumT = 0\n",
        "                    for tgt_state, probability in target_probs.items():             \n",
        "                        sumT += probability * Vp[tgt_state] #sum(T(s' a | s) * Vp(s')\n",
        "\n",
        "                    q += GAMMA * sumT #r(s) + GAMMA * sum(T(s' a | s) * Vp(s'))\n",
        "                    self.Q[state, action] = q #Q(s,a)\n",
        "\n",
        "            for state in self.S:\n",
        "                #Soft Maxent: V(s) = maxQ(s,a) + log(sum(exp(Q(s,a) - maxQ(s,a))))\n",
        "                best_action = self.select_action(state) \n",
        "                maxQ = self.Q[(state, best_action)] #maxQ(s,a)\n",
        "                tempV = 0\n",
        "                for action in self.actions: #sum()\n",
        "                    tempV += math.exp(self.Q[state, action] - maxQ) #exp(Q(s,a) - maxQ(s,a)\n",
        "                self.V[state] = maxQ + math.log(tempV) #update V(s)\n",
        "                        \n",
        "            diff = []\n",
        "            for state in self.S:\n",
        "                #check if diff between V and vp V  is within convergence critera\n",
        "                diff.append(abs(self.V[state] - Vp[state]))\n",
        "            diffMax = max(diff)\n",
        "\n",
        "            \"\"\"\n",
        "            formatlist = [i, diffMax]\n",
        "            print(\"Absoloute difference between V(s) & Vp on iteration {} is {}\".format(*formatlist))\n",
        "            i += 1\n",
        "            \"\"\"\n",
        "            \n",
        "            if diffMax < 0.01:\n",
        "                converged = True\n",
        "\n",
        "\n",
        "    def find_optimal_policy(self):\n",
        "        policy = [None] * len(self.S) #initialise empty policy\n",
        "        for state in self.S: #for each state\n",
        "            best_action = self.select_action(state) #select action with highest utility\n",
        "            policy[state] = best_action #set this as optimal action\n",
        "        return policy\n",
        "\n",
        "\n",
        "\n",
        "    def sample_paths(self, policy, no_paths):\n",
        "        paths = []\n",
        "        path = []\n",
        "        self.currentstate = self.S[0]\n",
        "        while (len(paths) != no_paths):\n",
        "            outcomes = self.T[self.currentstate, policy[self.currentstate]].values() #get probability of target states given action from policy as dict value obj\n",
        "            probs = [] #empty list\n",
        "            for item in outcomes:\n",
        "                probs.append(item) #populate probs list with outcomes \n",
        "            probs = np.array(probs) #cast probs list to np.array\n",
        "            newstatearray = np.random.multinomial(1, probs).tolist() #get new state array e.g [0,1,0]\n",
        "            new_state = newstatearray.index(1) #get actual state\n",
        "            path.append(policy[new_state]) #add action to path\n",
        "            endstate = self.endstate\n",
        "            if (new_state == endstate): #if at terminal state\n",
        "                #if (path not in paths and path != [1,1,2]): \n",
        "                paths.append(path) #add path to paths\n",
        "                path = [] #reset path\n",
        "                self.currentstate = self.S[0] #reset to start state\n",
        "            else:\n",
        "                self.currentstate = new_state \n",
        "\n",
        "        return paths\n",
        "\n",
        "    "
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "s4sN6oertPCK"
      },
      "source": [
        "#global helper functions\n",
        "\n",
        "def NormalizeData(data):\n",
        "    #normalises data to be within 0-1\n",
        "    return (data - np.min(data)) / (np.max(data) - np.min(data))\n",
        "\n",
        "def Normalize_rewards_to_1(data):\n",
        "    sum_val = sum(data)\n",
        "    for item in data:\n",
        "        index = data.index(item) #get index \n",
        "        new_item = item / sum_val\n",
        "        data[index] = new_item\n",
        "    return data"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twhX9COB8O6"
      },
      "source": [
        "#Likelihood & Hillclimb Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P69SYhk5WPNC"
      },
      "source": [
        "def likelihood(paths, rewards):\n",
        "    new_values = Normalize_rewards_to_1(list(rewards.values()))\n",
        "    for i in range(len(rewards)):\n",
        "        rewards[i] = new_values[i]\n",
        "    agent = MDP()\n",
        "    agent.init()\n",
        "    agent.rewards = rewards\n",
        "    agent.value_iteration_for_Q()\n",
        "    agent.find_optimal_policy()\n",
        "    likelihoodList = [] \n",
        "\n",
        "    for path in paths:\n",
        "        actionLikelihoodList = [None] * len(path) #empty list length of path\n",
        "        state = agent.S[0] \n",
        "        i = 0\n",
        "        for step in path:\n",
        "            action = step\n",
        "            state = max(agent.T[(state, action)].items(), key=operator.itemgetter(1))[0]  #update current state given action from path\n",
        "            actionLikelihoodList[i] = agent.Q[(state,action)]  - agent.V[state]#Q^Rstate,action −V^Rstate\n",
        "            i += 1\n",
        "        likelihoodList.append(sum(actionLikelihoodList))\n",
        "    return sum(likelihoodList)/len(likelihoodList)*100 "
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UlPDX_JYgNCX"
      },
      "source": [
        "def stochastic_hillclimb(paths, threshold):\n",
        "    Start = -1 #lower reward limit\n",
        "    Stop = 1 #upper reward limit\n",
        "    limit = 3 #number of states we are estimating rewards for \n",
        "    likelihood_threshold = threshold\n",
        "    threshold_reached = False \n",
        "    final_rewards = collections.defaultdict(float) #reward dict to be returned\n",
        "    dict_rewards = collections.defaultdict(float) #for inital likelihood calc\n",
        "    sigma = 0.3 #set standard deviation\n",
        "    sampledRewards = [random.uniform(Start, Stop) for iter in range(limit)] #initial random list of rewards\n",
        "\n",
        "    while(sum(sampledRewards) <= 0.0):\n",
        "        sampledRewards = [random.uniform(Start, Stop) for iter in range(limit)] #ensure random list of rewards are non-zero\n",
        "\n",
        "    sampledRewards = Normalize_rewards_to_1(sampledRewards) #ensure rewards sum to 1\n",
        "    dict_rewards = {0: sampledRewards[0], 1: sampledRewards[1], 2: sampledRewards[2]}\n",
        "    previousLikelihood = likelihood(paths, dict_rewards)\n",
        "\n",
        "    rewards_list_for_plotting = []\n",
        "    iterations = []\n",
        "    i = 1\n",
        "\n",
        "    while not threshold_reached:\n",
        "\n",
        "        #sample each states reward\n",
        "        state0Reward = random.normalvariate(sampledRewards[0], sigma) \n",
        "        state1Reward = random.normalvariate(sampledRewards[1], sigma) \n",
        "        state2Reward = random.normalvariate(sampledRewards[2], sigma)\n",
        "\n",
        "        #normalise rewards so they sums to one\n",
        "        normalised_rewards = Normalize_rewards_to_1([state0Reward, state1Reward, state2Reward])\n",
        "\n",
        "        rewards = {0: normalised_rewards[0], 1: normalised_rewards[1], 2: normalised_rewards[2]} #map to dictionary {state:rewards}\n",
        "        rewards_likelihood = likelihood(paths, rewards) #get likelihood of current reward function\n",
        "        \n",
        "        \n",
        "        \"\"\"\n",
        "        formatlist = [rewards, rewards_likelihood]\n",
        "        print(\"Rewards: {},   likeihood: {}\".format(*formatlist))\n",
        "        \"\"\"\n",
        "\n",
        "\n",
        "        #gather metrics for plotting\n",
        "        rewards_list_for_plotting.append(rewards_likelihood)\n",
        "        iterations.append(i)\n",
        "        i+=1\n",
        "\n",
        "        if rewards_likelihood > likelihood_threshold: #once threshold met\n",
        "            #terminate and return\n",
        "            threshold_reached = True\n",
        "            final_rewards = rewards\n",
        "\n",
        "    rewards_list_for_plotting = (rewards_list_for_plotting-np.mean(rewards_list_for_plotting))/np.std(rewards_list_for_plotting)\n",
        "    plt.scatter(iterations,rewards_list_for_plotting)\n",
        "    plt.plot(iterations,rewards_list_for_plotting)\n",
        "    plt.xlabel('Iterations')\n",
        "    plt.ylabel('Likelihood')\n",
        "    #plt.axhline(y=likelihood_threshold, color='r', linestyle=':')\n",
        "\n",
        "\n",
        "    return final_rewards"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mMJnw-bYA0Q"
      },
      "source": [
        "#Main"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dVnUMJuhofcw"
      },
      "source": [
        "#Initialising MDP\n",
        "agent = MDP()\n",
        "agent.init()\n",
        "agent.value_iteration_for_Q()\n",
        "optimal_policy = agent.find_optimal_policy()"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wL3Z-jIUomQ5"
      },
      "source": [
        "#Sample paths\n",
        "no_of_paths = 2000\n",
        "paths = agent.sample_paths(optimal_policy, no_of_paths)"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IAc40mV0YAg_",
        "outputId": "88eec3d6-f4ce-4e22-b217-931a1b8d8fca",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 349
        }
      },
      "source": [
        "printlist = [agent.rewards, likelihood(paths, agent.rewards), optimal_policy]\n",
        "priorrewards = agent.rewards\n",
        "print(\"\\nThe true reward function {}\\nhas likelihood of {} and optimal policy {}\".format(*printlist))\n",
        "\n",
        "formatlist = [no_of_paths, paths]\n",
        "print(\"\\nThe {} sampled paths are: {}\".format(*formatlist))\n",
        "\n",
        "print(\"\\nThe Q under optimal reward func is: \\n\")\n",
        "pprint.pprint(agent.Q)"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "The true reward function {0: -0.5, 1: 0.0, 2: 1.5}\n",
            "has likelihood of -235.4929968454503 and optimal policy [1, 1, 2]\n",
            "\n",
            "The 2000 sampled paths are: [[2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [2], [2], [2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [2], [2], [1, 1, 2], [2], [1, 2], [2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 2], [2], [2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 2], [1, 2], [2], [1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 1, 2], [2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [2], [2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [2], [2], [2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 1, 1, 1, 1, 1, 1, 1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [2], [2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 1, 1, 2], [2], [2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [2], [2], [2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [2], [2], [2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [2], [2], [1, 1, 2], [1, 2], [2], [1, 1, 1, 1, 2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [2], [2], [2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [2], [1, 1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [2], [2], [1, 2], [2], [1, 1, 1, 2], [2], [2], [1, 1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 1, 1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 2], [2], [2], [2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [2], [2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 1, 1, 1, 1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [2], [2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [2], [2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [2], [2], [2], [2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 1, 1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [2], [2], [1, 1, 1, 1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [1, 1, 1, 1, 1, 1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 1, 1, 1, 1, 1, 2], [2], [1, 1, 1, 1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [1, 1, 1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 1, 1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 1, 2], [2], [2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 1, 1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [1, 1, 2], [2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 1, 2], [2], [2], [2], [1, 1, 2], [1, 2], [1, 2], [2], [2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [2], [1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 1, 1, 2], [1, 2], [1, 2], [2], [1, 2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 1, 1, 1, 2], [2], [2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [1, 2], [2], [1, 2], [1, 2], [1, 1, 2], [2], [1, 1, 2], [1, 2], [2]]\n",
            "\n",
            "The Q under optimal reward func is: \n",
            "\n",
            "defaultdict(<class 'float'>,\n",
            "            {(0, 0): 11.439735391520673,\n",
            "             (0, 1): 12.1264650451663,\n",
            "             (0, 2): 11.439735391520673,\n",
            "             (1, 0): 11.939735391520673,\n",
            "             (1, 1): 13.274384306939796,\n",
            "             (1, 2): 12.6264650451663,\n",
            "             (2, 0): 13.904023559263113,\n",
            "             (2, 1): 13.439735391520673,\n",
            "             (2, 2): 14.774384306939796})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iZhu8_QEzVXn",
        "outputId": "2a1ba280-158c-41d1-b005-a8edeccab15a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 364
        }
      },
      "source": [
        "optimal_rewards = stochastic_hillclimb(paths, -235.49)\n",
        "agent.rewards = optimal_rewards\n",
        "agent.value_iteration_for_Q()\n",
        "formatlist = [optimal_rewards, likelihood(paths, optimal_rewards), agent.find_optimal_policy()]\n",
        "print(\"\\nHill climb converges at reward function {}\\nwith a likelihood of {} and optimal policy {}\".format(*formatlist))\n",
        "print(\"\\nThe Q under found reward func is: \\n\")\n",
        "pprint.pprint(agent.Q)\n",
        "agent.rewards = priorrewards"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-20-a0120ed24e12>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0moptimal_rewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstochastic_hillclimb\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m235.49\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptimal_rewards\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue_iteration_for_Q\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mformatlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0moptimal_rewards\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moptimal_rewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfind_optimal_policy\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"\\nHill climb converges at reward function {}\\nwith a likelihood of {} and optimal policy {}\"\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mformatlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-5-473035d4641d>\u001b[0m in \u001b[0;36mstochastic_hillclimb\u001b[0;34m(paths, threshold)\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m         \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormalised_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormalised_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mnormalised_rewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m2\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m}\u001b[0m \u001b[0;31m#map to dictionary {state:rewards}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 34\u001b[0;31m         \u001b[0mrewards_likelihood\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlikelihood\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpaths\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#get likelihood of current reward function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-4-1424ebfb7977>\u001b[0m in \u001b[0;36mlikelihood\u001b[0;34m(paths, rewards)\u001b[0m\n\u001b[1;32m     19\u001b[0m             \u001b[0mactionLikelihoodList\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mQ\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m  \u001b[0;34m-\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mV\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;31m#Q^Rstate,action −V^Rstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m             \u001b[0mi\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m         \u001b[0mlikelihoodList\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mactionLikelihoodList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlikelihoodList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlikelihoodList\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VZfaPY8sQ4Cd"
      },
      "source": [
        "#Testing Reward Func"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vU5AjqmJcXki",
        "outputId": "d3285079-b653-44b3-fbbd-e6ff691ca1f2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 82
        }
      },
      "source": [
        "#edit to test diff reward functions    \n",
        "agent = MDP()\n",
        "agent.init()\n",
        "agent.value_iteration_for_Q()\n",
        "rewardfunc = {0: -0.5, 1: 0.0, 2: 1.5}\n",
        "rewardfunc1 = {0: -0.5*20, 1: 0.0*20, 2: 1.5*20}\n",
        "LH = likelihood(paths, rewardfunc)\n",
        "LH1 = likelihood(paths, rewardfunc1)\n",
        "print(\"Likelihood of true reward func is {}\".format(LH))\n",
        "print(\"\\n\\nLikelihood of test reward func is {}\".format(LH1))"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Likelihood of true reward func is -232.9892572047436\n",
            "\n",
            "\n",
            "Likelihood of test reward func is -232.9892572047436\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYTWctxrvpGi"
      },
      "source": [
        "# **ignore** *Copy Paste Dump* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzvrDalNvrfq"
      },
      "source": [
        "\"\"\"\n",
        "threshold = 0.0001\n",
        "action = 0\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =100\n",
        "\n",
        "        #hard coded R\n",
        "        self.R_hardcoded = {(0, 0, 0): 0.0, \n",
        "                  (0, 0, 1): 0.0, \n",
        "                  (0, 0, 2): 1.0,\n",
        "                  (0, 1, 0): 0.0,\n",
        "                  (0, 1, 1): 0.0, \n",
        "                  (0, 1, 2): 1.0,\n",
        "                  (0, 2, 0): 0.0,\n",
        "                  (0, 2, 1): 0.0,\n",
        "                  (1, 0, 0): 0.0, \n",
        "                  (1, 0, 1): 0.0, \n",
        "                  (1, 0, 2): 1.0,\n",
        "                  (1, 1, 0): 0.0,\n",
        "                  (1, 1, 1): 0.0, \n",
        "                  (1, 1, 2): 1.0,\n",
        "                  (1, 2, 0): 0.0,\n",
        "                  (1, 2, 1): 0.0,\n",
        "                  (1, 2, 2): 1.0,\n",
        "                  (2, 0, 0): 0.0, \n",
        "                  (2, 0, 1): 0.0, \n",
        "                  (2, 0, 2): 1.0,\n",
        "                  (2, 1, 0): 0.0,\n",
        "                  (2, 1, 1): 0.0, \n",
        "                  (2, 1, 2): 1.0,\n",
        "                  (2, 2, 0): 0.0,\n",
        "                  (2, 2, 1): 0.0,\n",
        "                  (2, 2, 2): 1.0}\n",
        "\n",
        "        #hard coded T\n",
        "\n",
        "def step(self, action):\n",
        "        isdone = False\n",
        "        if action == 2:\n",
        "            currentstate = self.currentstate #remain at current state\n",
        "        elif action == 1:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)+1] #take step to right\n",
        "            except IndexError:\n",
        "                currentstate = self.actions[0]\n",
        "        else:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)-1] #take step to left\n",
        "            except IndexError:\n",
        "                currentstate = self.currentstate\n",
        "        if currentstate == self.endstate:\n",
        "            isdone = True\n",
        "        else:\n",
        "            isdone = False\n",
        "\n",
        "        return currentstate, self.rewards[currentstate], isdone\n",
        "        \n",
        "        \n",
        "        #single path function\n",
        "        def likelihood1(policy, agent):\n",
        "            likelihoodList = [None] * len(policy)\n",
        "            for i in range(len(policy)):\n",
        "                state = int(i)\n",
        "                action = int(policy[i])    \n",
        "                likelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "            return sum(likelihoodList) / len(likelihoodList)\n",
        "\n",
        "            #create random policies for testing\n",
        "actions = [0, 1, 2, 3]\n",
        "nonoptimal1 = [None] * 16\n",
        "nonoptimal2 = [None] * 16\n",
        "nonoptimal3 = [None] * 16\n",
        "policies = [nonoptimal1, nonoptimal2, nonoptimal3]\n",
        "for policy in policies:\n",
        "    for i in range(len(policy)):\n",
        "        policy[i] = random.choice(actions)\n",
        "\n",
        "#create random reward functions for testing\n",
        "nonoptimal_rewardfunc1 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc2 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc3 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "\n",
        "\n",
        "#get optimal reward function\n",
        "q_table = create_q_table(env) \n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal policy with random reward functions ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal reward function is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal reward function 1 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc1)))\n",
        "print(\"Likelihood for non optimal reward function 2 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc2)))\n",
        "print(\"Likelihood for non optimal reward function 3 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc3)))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal reward function with random policies ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal policy  is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal policy 1 is {}\".format(likelihood(nonoptimal1, q_table)))\n",
        "print(\"Likelihood for non optimal policy 2 is {}\".format(likelihood(nonoptimal2, q_table)))\n",
        "print(\"Likelihood for non optimal policy 3 is {}\".format(likelihood(nonoptimal3, q_table)))\n",
        "print(\"\\n\")\n",
        "                \n",
        "\"\"\"\n",
        "\n",
        "def likelihood2(paths, agent):\n",
        "    likelihoodList = []\n",
        "    for path in paths:\n",
        "        actionLikelihoodList = [None] * len(path)\n",
        "        for i in range(len(path)):\n",
        "            state = int(i)\n",
        "            action = int(path[i])    \n",
        "            actionLikelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "        likelihoodList.append(sum(actionLikelihoodList) / len(actionLikelihoodList))\n",
        "    return likelihoodList\n",
        "\n",
        "\"\"\"\n",
        "likelihoods = likelihood(paths, agent) #function call\n",
        "\n",
        "#formatting output\n",
        "print(\"The paths are: {}\".format(paths))\n",
        "for l in likelihoods:\n",
        "    index = likelihoods.index(l)\n",
        "    value = likelihoods[index]\n",
        "    formatlist = [index+1, value]\n",
        "    print(\"\\nLikelihood for path {} is {}\".format(*formatlist))\n",
        "\n",
        "print(\"\\nThe optimal policy is path {}\".format(likelihoods.index(max(likelihoods))+1))\n",
        "\n",
        "#take R as input and compute values in function\n",
        "\"\"\"\n",
        "\n",
        "def likelihood2(paths, rewards):\n",
        "    listoflikelihoodlists = []\n",
        "    for reward in rewards:\n",
        "        agent = MDP()\n",
        "        agent.init()\n",
        "        agent.rewards = reward\n",
        "        agent.play_n_random_steps(N)\n",
        "        agent.value_iteration_for_Q()\n",
        "        agent.find_optimal_policy()\n",
        "        likelihoodList = []\n",
        "        for path in paths:\n",
        "            actionLikelihoodList = [None] * len(path)\n",
        "            for i in range(len(path)):\n",
        "                state = int(i)\n",
        "                action = int(path[i])    \n",
        "                actionLikelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "            likelihoodList.append(sum(actionLikelihoodList) / len(actionLikelihoodList))\n",
        "        listoflikelihoodlists.append(likelihoodList)\n",
        "    \n",
        "    \n",
        "    return listoflikelihoodlists\n",
        "\"\"\"\n",
        "#0: 0.0, 1: 0.2, 2: 1.0\n",
        "likelihoods = likelihood(paths, {0: 0.0, 1: 0.2, 2: 1.0}) #function call\n",
        "formatlist = [paths, {0: 0.0, 1: 0.2, 2: 1.0}, likelihoods]\n",
        "print(\"The likelihoods of paths: \\n{}\\nunder the rewards: \\n{}\\nare: \\n\\n{}\\n\\n\".format(*formatlist))\n",
        "\n",
        "#0: 0.0, 1: 0.0, 2: 1.0\n",
        "likelihoods = likelihood(paths, {0: 0.0, 1: 0.0, 2: 1.0}) #function call\n",
        "formatlist = [paths, {0: 0.0, 1: 0.0, 2: 1.0}, likelihoods]\n",
        "print(\"The likelihoods of paths: \\n{}\\nunder the rewards: \\n{}\\nare: \\n\\n{}\\n\\n\".format(*formatlist))\n",
        "\n",
        "#0: 0.0, 1: 0.8, 2: 1.0\n",
        "likelihoods = likelihood(paths, {0: 0.0, 1: 0.8, 2: 1.0}) #function call\n",
        "formatlist = [paths, {0: 0.0, 1: 0.8, 2: 1.0}, likelihoods]\n",
        "print(\"The likelihoods of paths: \\n{}\\nunder the rewards: \\n{}\\nare: \\n\\n{}\\n\\n\".format(*formatlist))\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "rewards = [{0: 0.0, 1: 0.2, 2: 1.0},\n",
        "{0: 0.0, 1: 0.0, 2: 1.0},\n",
        "{0: 0.0, 1: 0.8, 2: 1.0},\n",
        "{0: 0.0, 1: 1.0, 2: 2.0}]\n",
        "\n",
        "\n",
        "listoflikelihoodlists = likelihood1(paths, rewards)\n",
        "\n",
        "print(\"\\n\\n\\nSum of likelihoods for each reward func are:\")\n",
        "\n",
        "for list in listoflikelihoodlists:\n",
        "    print(sum(list))\n",
        "\"\"\"\n",
        "\n",
        "'''\n",
        "def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = random.choice(self.actions) #select random action\n",
        "            new_state, reward, is_done = self.step(action) #take random action\n",
        "            self.R[(self.currentstate, action, new_state)] = reward #populate R with findings \n",
        "            self.currentstate = new_state #increment state\n",
        "        self.currentstate = self.S[0] #reset starting state\n",
        "'''\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Plr6WNpv6WZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}