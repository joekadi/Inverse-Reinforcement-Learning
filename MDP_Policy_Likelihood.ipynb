{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP_Policy_Likelihood.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "Sx2NJ1VN9DGG",
        "Z-cf5KwmJP79",
        "XYTWctxrvpGi"
      ]
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.4"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sx2NJ1VN9DGG"
      },
      "source": [
        "# Imports and Installs"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ygRt0pVU9H--",
        "outputId": "9452cc41-564a-4639-e310-a0746d2714d5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        }
      },
      "source": [
        "#Installs & Imports\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install torchvision\n",
        "import gym \n",
        "from gym import spaces\n",
        "import collections\n",
        "import pprint\n",
        "import torch\n",
        "import numpy as np\n",
        "import random\n",
        "import operator"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCSjGFN7MTfj"
      },
      "source": [
        "#Initialising MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps67o2Xl_GjO"
      },
      "source": [
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =1000\n",
        "class MDP:\n",
        "    def init(self):\n",
        "        self.S = [0, 1, 2]\n",
        "        self.endstate = self.S[-1]\n",
        "        self.gamma = 0.99\n",
        "        self.actions = [0, 1, 2] # 0 = BACK, 1 = FORWARD, 2 = STAY\n",
        "        self.currentstate = self.actions[0]\n",
        "        self.rewards = {0: 0.0,\n",
        "                        1: 0.2,\n",
        "                        2: 1.0}\n",
        "        self.T = {\n",
        "            (0, 0):{0: 0.8, 1: 0.1, 2: 0.1},  \n",
        "            (0, 1): {0: 0.1, 1: 0.7, 2: 0.2}, \n",
        "            (0, 2): {0: 0.8, 1: 0.1}, \n",
        "            (1, 0): {0: 0.7, 1: 0.2, 2: 0.1}, \n",
        "            (1, 1): {0: 0.1, 1: 0.1, 2: 0.8}, \n",
        "            (1, 2): {0: 0.1, 1: 0.8, 2: 0.1},\n",
        "            (2, 0): {0: 0.1, 1: 0.8, 2: 0.1}, \n",
        "            (2, 1): {0: 0.8, 1: 0.1, 2: 0.1},\n",
        "            (2, 2): {0: 0.1, 1: 0.1, 2: 0.8}}\n",
        "\n",
        "        self.R = collections.defaultdict(float)\n",
        "        self.values = collections.defaultdict(float)\n",
        "        \n",
        "    def step(self, action):\n",
        "        isdone = False\n",
        "        new_state = max(self.T[(self.currentstate, action)].items(), key=operator.itemgetter(1))[0]\n",
        "        if new_state == self.endstate:\n",
        "            isdone = True\n",
        "        else:\n",
        "            isdone = False\n",
        "\n",
        "        return new_state, self.rewards[new_state], isdone\n",
        "       \n",
        "    def select_action(self, state):\n",
        "        #helper function to select action with highest utility\n",
        "        best_action, best_value = None, None\n",
        "        for action in range(len(self.actions)):\n",
        "            action_value = self.values[(state, action)]\n",
        "            if best_value is None or best_value < action_value:\n",
        "                best_value = action_value\n",
        "                best_action = action\n",
        "        return best_action\n",
        "\n",
        "    def get_state_utility(self, state):\n",
        "        utility = 0\n",
        "        for action in self.actions:\n",
        "            utility += self.values[state, action]\n",
        "\n",
        "        return utility/len(agent.actions)\n",
        "\n",
        "        \n",
        "\n",
        "    def play_n_random_steps(self, count):\n",
        "        for _ in range(count):\n",
        "            action = random.choice(self.actions)\n",
        "            new_state, reward, is_done = self.step(action)\n",
        "            #print(\"State: {}\".format(self.currentstate), \"Action: {}\".format(action), \"New State{}\".format(new_state), \"Reward {}\".format(reward) )\n",
        "            self.R[(self.currentstate, action, new_state)] = reward\n",
        "            #self.T[(self.currentstate, action)][new_state] += 1 \n",
        "            \n",
        "            self.currentstate = new_state\n",
        "\n",
        "            \"\"\"\n",
        "            if is_done:\n",
        "                self.currentstate = self.S[0]\n",
        "            else: \n",
        "                self.currentstate = new_state\n",
        "            \"\"\"\n",
        "\n",
        "    def value_iteration_for_Q(self):\n",
        "        for state in self.S:\n",
        "            for action in self.actions:\n",
        "                action_value = 0.0\n",
        "                target_probs = self.T[(state, action)]\n",
        "                total = sum(target_probs.values())\n",
        "                for tgt_state, probability in target_probs.items():\n",
        "                    key = (state, action, tgt_state)\n",
        "                    reward = self.R[key]\n",
        "                    best_action = self.select_action(tgt_state)\n",
        "                    val = reward + GAMMA * self.values[(tgt_state, best_action)]\n",
        "                    action_value += (probability / total) * val\n",
        "                self.values[(state, action)] = action_value\n",
        "\n",
        "    def find_optimal_policy(self):\n",
        "        policy = [None] * len(self.S)\n",
        "        for state in self.S:\n",
        "            best_action = self.select_action(state)\n",
        "            policy[state] = best_action\n",
        "        return policy"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTDne9lbG0JJ"
      },
      "source": [
        "#Solving MDP"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9kavHZ3cTaC",
        "outputId": "4185b36a-8fd7-43aa-d97c-9a5143562267",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "agent = MDP()\n",
        "agent.init()\n",
        "agent.play_n_random_steps(N)\n",
        "agent.value_iteration_for_Q()\n",
        "print(agent.find_optimal_policy())"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twhX9COB8O6"
      },
      "source": [
        "#Likelihood Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm8KOzjPYS0K",
        "outputId": "04b215bd-b12f-42e0-b9d0-934d4d3b04b8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "def likelihood(paths, agent):\n",
        "    for path in paths:\n",
        "        likelihoodList = [None] * len(path)\n",
        "        for i in range(len(path)):\n",
        "            state = int(i)\n",
        "            action = int(path[i])    \n",
        "            likelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "        format_list = [paths.index(path), sum(likelihoodList) / len(likelihoodList)]\n",
        "        print(\"Likelihood for path {} is {}\".format(*format_list)) \n",
        "\n",
        "\n",
        "testpaths = [[1,1,2], [1,1,0,1], [2,2,1,1], [1,1,1,1,1,2]]\n",
        "\n",
        "likelihood(testpaths, agent)\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Likelihood for path 0 is 0.31365807066666673\n",
            "Likelihood for path 1 is 0.06706856320000003\n",
            "Likelihood for path 2 is -0.08422761619999994\n",
            "Likelihood for path 3 is -0.041959970799999975\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-cf5KwmJP79"
      },
      "source": [
        "#Testing likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJWElEbCJR6W",
        "outputId": "e04a7ad3-fd91-4c48-c07d-a365dfaad8ad",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        }
      },
      "source": [
        "#create random policies for testing\n",
        "actions = [0, 1, 2, 3]\n",
        "nonoptimal1 = [None] * 16\n",
        "nonoptimal2 = [None] * 16\n",
        "nonoptimal3 = [None] * 16\n",
        "policies = [nonoptimal1, nonoptimal2, nonoptimal3]\n",
        "for policy in policies:\n",
        "    for i in range(len(policy)):\n",
        "        policy[i] = random.choice(actions)\n",
        "\n",
        "#create random reward functions for testing\n",
        "nonoptimal_rewardfunc1 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc2 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc3 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "\n",
        "\n",
        "#get optimal reward function\n",
        "q_table = create_q_table(env) \n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal policy with random reward functions ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal reward function is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal reward function 1 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc1)))\n",
        "print(\"Likelihood for non optimal reward function 2 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc2)))\n",
        "print(\"Likelihood for non optimal reward function 3 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc3)))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal reward function with random policies ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal policy  is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal policy 1 is {}\".format(likelihood(nonoptimal1, q_table)))\n",
        "print(\"Likelihood for non optimal policy 2 is {}\".format(likelihood(nonoptimal2, q_table)))\n",
        "print(\"Likelihood for non optimal policy 3 is {}\".format(likelihood(nonoptimal3, q_table)))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'env' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-12-aad2df95c323>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m#create random reward functions for testing\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0mnonoptimal_rewardfunc1\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0mnonoptimal_rewardfunc2\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0mnonoptimal_rewardfunc3\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrand\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mn\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'env' is not defined"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYTWctxrvpGi"
      },
      "source": [
        "# **ignore** *Copy Paste Dump* "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GzvrDalNvrfq",
        "outputId": "ca73f776-49e3-4e2f-ead3-8bce4be3f374",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 154
        }
      },
      "source": [
        "\"\"\"\n",
        "threshold = 0.0001\n",
        "action = 0\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =100\n",
        "\n",
        "        #hard coded R\n",
        "        self.R_hardcoded = {(0, 0, 0): 0.0, \n",
        "                  (0, 0, 1): 0.0, \n",
        "                  (0, 0, 2): 1.0,\n",
        "                  (0, 1, 0): 0.0,\n",
        "                  (0, 1, 1): 0.0, \n",
        "                  (0, 1, 2): 1.0,\n",
        "                  (0, 2, 0): 0.0,\n",
        "                  (0, 2, 1): 0.0,\n",
        "                  (1, 0, 0): 0.0, \n",
        "                  (1, 0, 1): 0.0, \n",
        "                  (1, 0, 2): 1.0,\n",
        "                  (1, 1, 0): 0.0,\n",
        "                  (1, 1, 1): 0.0, \n",
        "                  (1, 1, 2): 1.0,\n",
        "                  (1, 2, 0): 0.0,\n",
        "                  (1, 2, 1): 0.0,\n",
        "                  (1, 2, 2): 1.0,\n",
        "                  (2, 0, 0): 0.0, \n",
        "                  (2, 0, 1): 0.0, \n",
        "                  (2, 0, 2): 1.0,\n",
        "                  (2, 1, 0): 0.0,\n",
        "                  (2, 1, 1): 0.0, \n",
        "                  (2, 1, 2): 1.0,\n",
        "                  (2, 2, 0): 0.0,\n",
        "                  (2, 2, 1): 0.0,\n",
        "                  (2, 2, 2): 1.0}\n",
        "\n",
        "        #hard coded T\n",
        "\n",
        "def step(self, action):\n",
        "        isdone = False\n",
        "        if action == 2:\n",
        "            currentstate = self.currentstate #remain at current state\n",
        "        elif action == 1:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)+1] #take step to right\n",
        "            except IndexError:\n",
        "                currentstate = self.actions[0]\n",
        "        else:\n",
        "            try:\n",
        "                currentstate = self.actions[self.actions.index(self.currentstate)-1] #take step to left\n",
        "            except IndexError:\n",
        "                currentstate = self.currentstate\n",
        "        if currentstate == self.endstate:\n",
        "            isdone = True\n",
        "        else:\n",
        "            isdone = False\n",
        "\n",
        "        return currentstate, self.rewards[currentstate], isdone\n",
        "        \n",
        "        \n",
        "        #single path function\n",
        "        def likelihood1(policy, agent):\n",
        "            likelihoodList = [None] * len(policy)\n",
        "            for i in range(len(policy)):\n",
        "                state = int(i)\n",
        "                action = int(policy[i])    \n",
        "                likelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \n",
        "            return sum(likelihoodList) / len(likelihoodList)\n",
        "                \n",
        "\"\"\""
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nthreshold = 0.0001\\naction = 0\\nTEST_EPISODES = 20\\nREWARD_GOAL = 0.8\\nN =100\\n\\n        #hard coded R\\n        self.R_hardcoded = {(0, 0, 0): 0.0, \\n                  (0, 0, 1): 0.0, \\n                  (0, 0, 2): 1.0,\\n                  (0, 1, 0): 0.0,\\n                  (0, 1, 1): 0.0, \\n                  (0, 1, 2): 1.0,\\n                  (0, 2, 0): 0.0,\\n                  (0, 2, 1): 0.0,\\n                  (1, 0, 0): 0.0, \\n                  (1, 0, 1): 0.0, \\n                  (1, 0, 2): 1.0,\\n                  (1, 1, 0): 0.0,\\n                  (1, 1, 1): 0.0, \\n                  (1, 1, 2): 1.0,\\n                  (1, 2, 0): 0.0,\\n                  (1, 2, 1): 0.0,\\n                  (1, 2, 2): 1.0,\\n                  (2, 0, 0): 0.0, \\n                  (2, 0, 1): 0.0, \\n                  (2, 0, 2): 1.0,\\n                  (2, 1, 0): 0.0,\\n                  (2, 1, 1): 0.0, \\n                  (2, 1, 2): 1.0,\\n                  (2, 2, 0): 0.0,\\n                  (2, 2, 1): 0.0,\\n                  (2, 2, 2): 1.0}\\n\\n        #hard coded T\\n\\ndef step(self, action):\\n        isdone = False\\n        if action == 2:\\n            currentstate = self.currentstate #remain at current state\\n        elif action == 1:\\n            try:\\n                currentstate = self.actions[self.actions.index(self.currentstate)+1] #take step to right\\n            except IndexError:\\n                currentstate = self.actions[0]\\n        else:\\n            try:\\n                currentstate = self.actions[self.actions.index(self.currentstate)-1] #take step to left\\n            except IndexError:\\n                currentstate = self.currentstate\\n        if currentstate == self.endstate:\\n            isdone = True\\n        else:\\n            isdone = False\\n\\n        return currentstate, self.rewards[currentstate], isdone\\n        \\n        \\n        #single path function\\n        def likelihood1(policy, agent):\\n            likelihoodList = [None] * len(policy)\\n            for i in range(len(policy)):\\n                state = int(i)\\n                action = int(policy[i])    \\n                likelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) \\n            return sum(likelihoodList) / len(likelihoodList)\\n                \\n'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-Plr6WNpv6WZ"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}