{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MDP_Policy_Likelihood.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CCSjGFN7MTfj",
        "colab_type": "text"
      },
      "source": [
        "#Initialising MDP & Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cxXTGWdU1jb0",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 292
        },
        "outputId": "8819ce96-3355-4608-fc36-617a70914b8f"
      },
      "source": [
        "#Installs & Imports\n",
        "!pip install scipy\n",
        "!pip install matplotlib\n",
        "!pip install numpy\n",
        "!pip install torchvision\n",
        "import gym \n",
        "from gym import spaces\n",
        "import collections\n",
        "import pprint\n",
        "import torch\n",
        "import numpy as np\n",
        "import random"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: scipy in /usr/local/lib/python3.6/dist-packages (1.4.1)\n",
            "Requirement already satisfied: numpy>=1.13.3 in /usr/local/lib/python3.6/dist-packages (from scipy) (1.18.5)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.6/dist-packages (3.2.2)\n",
            "Requirement already satisfied: numpy>=1.11 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.18.5)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (1.2.0)\n",
            "Requirement already satisfied: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.4.7)\n",
            "Requirement already satisfied: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (2.8.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib) (0.10.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.1->matplotlib) (1.15.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (1.18.5)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.6/dist-packages (0.7.0+cu101)\n",
            "Requirement already satisfied: pillow>=4.1.1 in /usr/local/lib/python3.6/dist-packages (from torchvision) (7.0.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.18.5)\n",
            "Requirement already satisfied: torch==1.6.0 in /usr/local/lib/python3.6/dist-packages (from torchvision) (1.6.0+cu101)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.6/dist-packages (from torch==1.6.0->torchvision) (0.16.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ps67o2Xl_GjO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\"\"\"\n",
        "threshold = 0.0001\n",
        "action = 0\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =100\n",
        "\"\"\"\n",
        "\n",
        "GAMMA = 0.9\n",
        "TEST_EPISODES = 20\n",
        "REWARD_GOAL = 0.8\n",
        "N =100\n",
        "class MDP:\n",
        "  def init(self):\n",
        "    self.S = [0, 1, 2]\n",
        "    self.gamma = 0.99\n",
        "    self.actions = [0, 1, 2] # 0 = BACK, 1 = FORWARD, 2 = STAY\n",
        "\n",
        "\n",
        "    self.T = {(0, 0):{0: 0.8, 1: 0.1, 2: 0.1},  \n",
        "              (0, 1): {0: 0.1, 1: 0.7, 2: 0.2}, \n",
        "              (0, 2): {0: 0.8, 1: 0.1}, \n",
        "              (1, 0): {0: 0.7, 1: 0.2, 2: 0.1}, \n",
        "              (1, 1): {0: 0.1, 1: 0.1, 2: 0.8}, \n",
        "              (1, 2): {0: 0.1, 1: 0.8, 2: 0.1},\n",
        "              (2, 0): {0: 0.1, 1: 0.8, 2: 0.1}, \n",
        "              (2, 1): {0: 0.8, 1: 0.1, 2: 0.1},\n",
        "              (2, 2): {0: 0.1, 1: 0.1, 2: 0.8}}\n",
        "\n",
        "    self.R = {(0, 0, 0): 0.0, \n",
        "              (0, 0, 1): 0.0, \n",
        "              (0, 0, 2): 1.0,\n",
        "              (0, 1, 0): 0.0,\n",
        "              (0, 1, 1): 0.0, \n",
        "              (0, 1, 2): 1.0,\n",
        "              (0, 2, 0): 0.0,\n",
        "              (0, 2, 1): 0.0,\n",
        "              (1, 0, 0): 0.0, \n",
        "              (1, 0, 1): 0.0, \n",
        "              (1, 0, 2): 1.0,\n",
        "              (1, 1, 0): 0.0,\n",
        "              (1, 1, 1): 0.0, \n",
        "              (1, 1, 2): 1.0,\n",
        "              (1, 2, 0): 0.0,\n",
        "              (1, 2, 1): 0.0,\n",
        "              (1, 2, 2): 1.0,\n",
        "              (2, 0, 0): 0.0, \n",
        "              (2, 0, 1): 0.0, \n",
        "              (2, 0, 2): 1.0,\n",
        "              (2, 1, 0): 0.0,\n",
        "              (2, 1, 1): 0.0, \n",
        "              (2, 1, 2): 1.0,\n",
        "              (2, 2, 0): 0.0,\n",
        "              (2, 2, 1): 0.0,\n",
        "              (2, 2, 2): 1.0}\n",
        "\n",
        "    self.values = collections.defaultdict(float)\n",
        "\n",
        "  def select_action(self, state):\n",
        "      #helper function to select action with highest utility\n",
        "      best_action, best_value = None, None\n",
        "      for action in range(len(self.actions)):\n",
        "          action_value = self.values[(state, action)]\n",
        "          if best_value is None or best_value < action_value:\n",
        "            best_value = action_value\n",
        "            best_action = action\n",
        "      return best_action\n",
        "\n",
        "  def get_state_utility(self, state):\n",
        "      utility = 0\n",
        "      for action in self.actions:\n",
        "          utility += self.values[state, action]\n",
        "\n",
        "      return utility/len(agent.actions)\n",
        "\n",
        "  def value_iteration_for_Q(self):\n",
        "      for state in self.S:\n",
        "          for action in self.actions:\n",
        "              action_value = 0.0\n",
        "              target_probs = self.T[(state, action)]\n",
        "              total = sum(target_probs.values())\n",
        "              for tgt_state, probability in target_probs.items():\n",
        "                  key = (state, action, tgt_state)\n",
        "                  reward = self.R[key]\n",
        "                  best_action = self.select_action(tgt_state)\n",
        "                  val = reward + GAMMA * self.values[(tgt_state, best_action)]\n",
        "                  action_value += (probability / total) * val\n",
        "              self.values[(state, action)] = action_value\n",
        "\n",
        "  def find_optimal_policy(self):\n",
        "    policy = [None] * len(self.S)\n",
        "\n",
        "    for state in self.S:\n",
        "      best_action = self.select_action(state)\n",
        "      policy[state] = best_action\n",
        "    return policy"
      ],
      "execution_count": 142,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TTDne9lbG0JJ",
        "colab_type": "text"
      },
      "source": [
        "#Solve MDP, find optimal policy & sample random paths"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "i9kavHZ3cTaC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "15fcf0fc-7df9-42a0-bd1f-46dd40dfa2e8"
      },
      "source": [
        "agent = MDP()\n",
        "agent.init()\n",
        "agent.value_iteration_for_Q()\n",
        "print(agent.find_optimal_policy())"
      ],
      "execution_count": 143,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[1, 1, 2]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-twhX9COB8O6",
        "colab_type": "text"
      },
      "source": [
        "#Likelihood Function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YmlvwXv6CETh",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#for i:\n",
        "  #for t:\n",
        "    #s = state(i)\n",
        "    #a = action(i)\n",
        "    #likelihood[i] = Q(s,a) - V(s)\n",
        "#return likelihood\n",
        "\n",
        "def likelihood(policy, rewardfunc):\n",
        "  likelihoodList = [None] * len(policy)\n",
        "  for i in range(len(policy)):\n",
        "    state = int(i)\n",
        "    action = int(policy[i])\n",
        "    likelihoodList[i] = rewardfunc[state,action] - np.argmax(rewardfunc[state,:])\n",
        "  return sum(likelihoodList) / len(likelihoodList) "
      ],
      "execution_count": 144,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bm8KOzjPYS0K",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 255
        },
        "outputId": "66ef0bf1-3073-419f-82e8-b8c97e282f8b"
      },
      "source": [
        "#new likelihood function\n",
        "def likelihood1(policy, agent):\n",
        "  likelihoodList = [None] * len(policy)\n",
        "  for i in range(len(policy)):\n",
        "    state = int(i)\n",
        "    action = int(policy[i])\n",
        "    #likelihoodList[i] = agent.R[(state, action, max(agent.T[(0,1)].items(), key=operator.itemgetter(1))[0])] - np.argmax(agent.R[state,:,:])\n",
        "    \n",
        "    likelihoodList[i] = agent.values[state,action]  - agent.get_state_utility(state) #change this to ... sum probabilites of all actions in state / number of actions\n",
        "    #print(agent.get_state_utility(state))\n",
        "  return sum(likelihoodList) / len(likelihoodList)\n",
        "\n",
        "pprint.pprint(agent.values)\n",
        "print(likelihood1([1,1,2], agent))\n",
        "print(likelihood1([1,2,2], agent))\n",
        "print(likelihood1([2,2,2], agent))\n",
        "print(likelihood1([1,1,2], agent))\n",
        "\n",
        "\n"
      ],
      "execution_count": 152,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "defaultdict(<class 'float'>,\n",
            "            {(0, 0): 0.1,\n",
            "             (0, 1): 0.20900000000000002,\n",
            "             (0, 2): 0.16720000000000002,\n",
            "             (1, 0): 0.23167000000000004,\n",
            "             (1, 1): 0.8396603,\n",
            "             (1, 2): 0.723365416,\n",
            "             (2, 0): 0.723365416,\n",
            "             (2, 1): 0.39115231444000004,\n",
            "             (2, 2): 1.4152025265200001})\n",
            "0.2878858340666667\n",
            "0.24912087273333336\n",
            "0.2351875394\n",
            "0.2878858340666667\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mOAeo7emZ0WG",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        },
        "outputId": "ee1b99b7-d5b3-4c6a-d0d3-b0005253415b"
      },
      "source": [
        "import operator\n",
        "print(agent.T[(0,1)].items())"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "dict_items([(0, 0.1), (1, 0.7), (2, 0.2)])\n",
            "1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Z-cf5KwmJP79",
        "colab_type": "text"
      },
      "source": [
        "#Testing likelihood function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GJWElEbCJR6W",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 468
        },
        "outputId": "e04a7ad3-fd91-4c48-c07d-a365dfaad8ad"
      },
      "source": [
        "#create random policies for testing\n",
        "actions = [0, 1, 2, 3]\n",
        "nonoptimal1 = [None] * 16\n",
        "nonoptimal2 = [None] * 16\n",
        "nonoptimal3 = [None] * 16\n",
        "policies = [nonoptimal1, nonoptimal2, nonoptimal3]\n",
        "for policy in policies:\n",
        "  for i in range(len(policy)):\n",
        "    policy[i] = random.choice(actions)\n",
        "\n",
        "#create random reward functions for testing\n",
        "nonoptimal_rewardfunc1 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc2 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "nonoptimal_rewardfunc3 = np.random.rand(env.observation_space.n,env.action_space.n)\n",
        "\n",
        "\n",
        "#get optimal reward function\n",
        "q_table = create_q_table(env) \n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal policy with random reward functions ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal reward function is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal reward function 1 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc1)))\n",
        "print(\"Likelihood for non optimal reward function 2 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc2)))\n",
        "print(\"Likelihood for non optimal reward function 3 is {}\".format(likelihood(optimal_policy, nonoptimal_rewardfunc3)))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n*** Testing likelihood function using optimal reward function with random policies ***\\n\")\n",
        "print(\"---------------------------------------\")\n",
        "print(\"\\n\")\n",
        "print(\"Likelihood for optimal policy  is {}\".format(likelihood(optimal_policy, q_table)))\n",
        "print(\"Likelihood for non optimal policy 1 is {}\".format(likelihood(nonoptimal1, q_table)))\n",
        "print(\"Likelihood for non optimal policy 2 is {}\".format(likelihood(nonoptimal2, q_table)))\n",
        "print(\"Likelihood for non optimal policy 3 is {}\".format(likelihood(nonoptimal3, q_table)))\n",
        "print(\"\\n\")\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------------------------------------\n",
            "\n",
            "*** Testing likelihood function using optimal policy with random reward functions ***\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "Likelihood for optimal reward function is 40.124804168150696\n",
            "Likelihood for non optimal reward function 1 is -0.6772040634978659\n",
            "Likelihood for non optimal reward function 2 is -0.9997567588686376\n",
            "Likelihood for non optimal reward function 3 is -0.9593457577245044\n",
            "\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "*** Testing likelihood function using optimal reward function with random policies ***\n",
            "\n",
            "---------------------------------------\n",
            "\n",
            "\n",
            "Likelihood for optimal policy  is 40.124804168150696\n",
            "Likelihood for non optimal policy 1 is 20.786019607418783\n",
            "Likelihood for non optimal policy 2 is 29.617069756401513\n",
            "Likelihood for non optimal policy 3 is 23.71816216320028\n",
            "\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-UMgXVSuJZ5x",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "v1UoXlOPt2xU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}